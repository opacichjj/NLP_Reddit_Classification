{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASOIAF/GoT Reddit Posts - Pt. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># Feature Engineering and Baseline Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import TextCategorizer\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading our data\n",
    "data = pd.read_csv('./data/unique_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>parsed_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welcome to the Weekly Q &amp;amp; A! Feel free to ...</td>\n",
       "      <td>asoiaf</td>\n",
       "      <td>Welcome to the Weekly Q &amp;amp; A! Feel free to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's happened to all of us.\\n\\nYou come across...</td>\n",
       "      <td>asoiaf</td>\n",
       "      <td>It's happened to all of us.\\n\\nYou come across...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Something thats always bothered me is Tywin's ...</td>\n",
       "      <td>asoiaf</td>\n",
       "      <td>Something thats always bothered me is Tywin's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apologies if this has been posted before.\\n\\nI...</td>\n",
       "      <td>asoiaf</td>\n",
       "      <td>Apologies if this has been posted before.\\n\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One of the things I was sorry not to get more ...</td>\n",
       "      <td>asoiaf</td>\n",
       "      <td>One of the things I was sorry not to get more ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           post_text subreddit  \\\n",
       "0  Welcome to the Weekly Q &amp; A! Feel free to ...    asoiaf   \n",
       "1  It's happened to all of us.\\n\\nYou come across...    asoiaf   \n",
       "2  Something thats always bothered me is Tywin's ...    asoiaf   \n",
       "3  Apologies if this has been posted before.\\n\\nI...    asoiaf   \n",
       "4  One of the things I was sorry not to get more ...    asoiaf   \n",
       "\n",
       "                                         parsed_post  \n",
       "0  Welcome to the Weekly Q &amp; A! Feel free to ...  \n",
       "1  It's happened to all of us.\\n\\nYou come across...  \n",
       "2  Something thats always bothered me is Tywin's ...  \n",
       "3  Apologies if this has been posted before.\\n\\nI...  \n",
       "4  One of the things I was sorry not to get more ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## showing first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    939\n",
       "1    603\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## binarizing our subreddit category\n",
    "data['subreddit'] = data['subreddit'].map({'asoiaf':0,\n",
    "                               'gameofthrones':1})\n",
    "## showing success\n",
    "data['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>welcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>welcome to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the weekly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekly q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term\n",
       "0     welcome\n",
       "1           q\n",
       "2  welcome to\n",
       "3  the weekly\n",
       "4    weekly q"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing our custom stop words\n",
    "stop_list = pd.read_csv('./data/stop_list.csv')\n",
    "stop_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating our stop word/phrase list\n",
    "custom_stop = []\n",
    "for word in range(len(stop_list['term'])):\n",
    "    custom_stop.append(stop_list['term'][word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome', 'q', 'welcome to', 'the weekly', 'weekly q']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## printing our first 5\n",
    "custom_stop[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting our X and target(y) variables\n",
    "X = data['parsed_post']\n",
    "y = data['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.608949\n",
       "1    0.391051\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## showing our baseline percentage\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Building a Model with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Welcome to the Weekly Q &amp; A! Feel free to ask any questions you may have about the world of ASOIAF. No need to be bashful. Book and show questions are welcome; please say in your question if you would prefer to focus on the BOOKS, the SHOW, or BOTH.  And if you think you've got an answer to someone's question, feel free to lend them a hand!\",\n",
       " 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using a lambda function to turn all parsed_posts and subreddits into a Series containing tuples\n",
    "data['tuples'] = data.apply(\n",
    "    lambda row: (row['parsed_post'],row['subreddit']), axis=1)\n",
    "\n",
    "## turning the tuples into a list we'll use for training\n",
    "train = data['tuples'].tolist()\n",
    "\n",
    "## showing our first tuple\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOURCE: functions mainly manufactured from spacy documentation\n",
    "\n",
    "## creating a function that will load are training size and explicitly call our limit\n",
    "def load_data(limit=0, train_size=0.8):\n",
    "    \n",
    "    train_data = train  ## setting our training data\n",
    "    np.random.shuffle(train_data)  ## randomly shuffling our training data\n",
    "    \n",
    "    ## setting the limit based on function being passed\n",
    "    ## with current code it will be set to n_texts value below\n",
    "    train_data = train_data[-limit:]\n",
    "    \n",
    "    ## unzipping our tuple into two separate lists (texts and subreddit value/type)\n",
    "    texts, bool_val = zip(*train_data)\n",
    "    \n",
    "    ## setting our subreddit values to True or False\n",
    "    ## this will be used in the scoring portion of the 'evaluate' function\n",
    "    cats = [{'Game of Thrones': bool(y)} for y in bool_val]\n",
    "    split = int(len(train_data) * train_size)  ## splits our training data based on argument passed\n",
    "    \n",
    "    ## returns our text and category (True/False vals) test/train split\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "\n",
    "\n",
    "## creating a function that will score our data after instantiating our optimizer\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    \n",
    "    ## setting our docs for \n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    \n",
    "    ## setting empty values for our confusion matrix and scores\n",
    "    tp = 1e-8 # True positives , 1e-8 is there to ensure we never divide by zero\n",
    "    fp = 0 # False positives\n",
    "    fn = 0  # False negatives\n",
    "    tn = 0  # True negatives\n",
    "    \n",
    "    ## using our textcat.pipe on our tokenized docs to pull a number and it's associated value\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        \n",
    "        gold = cats[i] ## setting our 'gold_parse' to the indexed dict value of {'GoT': T or F} \n",
    "        \n",
    "        ## for_loop to extract boolean values of 'gold_parse' and doc 'score' from textcat.pipe\n",
    "        for bool_val, score in doc.cats.items():\n",
    "            \n",
    "            ## a series of if_statements that will increase count on our confusion matrix\n",
    "            ## when the each parameter for 'score' and 'bool_val' are met\n",
    "            if bool_val not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[bool_val] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[bool_val] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[bool_val] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[bool_val] >= 0.5:\n",
    "                fn += 1\n",
    "    \n",
    "    ## calculate and return the following scores\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)            \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return {'textcat_p': precision, \n",
    "            'textcat_r': recall, \n",
    "            'textcat_f': f_score, \n",
    "            'textcat_a': accuracy}\n",
    "\n",
    "## number of texts to train\n",
    "n_texts=len(data)\n",
    "\n",
    "## number of training iterations\n",
    "n_iter=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1542 examples (1233 training, 309 evaluation)\n"
     ]
    }
   ],
   "source": [
    "## this is the protocol for calling spaCy's TextCategorizer'textcat'\n",
    "## Code adapted from spaCy's documentation to create or add the necessary pipeline\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe('textcat')\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "# otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "## adding label to text classifier, used for Boolean comparison\n",
    "textcat.add_label('Game of Thrones')\n",
    "\n",
    "## this creates our train/test split from the 'load_data' function\n",
    "(train_texts, train_cats), (test_texts, test_cats) = load_data(limit=n_texts)  \n",
    "\n",
    "## prints details of line above, samples size and split\n",
    "print(f'Using {n_texts} examples ({len(train_texts)} training, {len(test_texts)} evaluation)')\n",
    "\n",
    "## takes 'train_text' and 'bool_val' from load_data function\n",
    "## this will get evaluated in the next block of code\n",
    "train_data = list(zip(train_texts,\n",
    "                      [{'cats': cats} for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "  LOSS     P       R       F       A    \n",
      " 19.42   0.941   0.926   0.933   0.948  \n",
      " 10.22   0.932   0.909   0.921   0.939  \n",
      "  8.43   0.922   0.884   0.903   0.926  \n",
      "  6.36   0.939   0.884   0.911   0.932  \n",
      "  4.55   0.946   0.868   0.905   0.929  \n",
      "  4.38    0.93   0.884   0.907   0.929  \n",
      "  4.66   0.922   0.876   0.898   0.922  \n",
      "  3.9    0.915   0.893   0.904   0.926  \n",
      "  4.22   0.902   0.917    0.91   0.929  \n",
      "  4.17   0.901   0.901   0.901   0.922  \n",
      "  3.5     0.91   0.917   0.914   0.932  \n",
      "  3.23   0.899   0.884   0.892   0.916  \n",
      "  3.19   0.893   0.893   0.893   0.916  \n",
      "  4.62   0.897   0.868   0.882   0.909  \n",
      "  3.14    0.9    0.893   0.896   0.919  \n",
      "  3.61   0.892   0.884   0.888   0.913  \n",
      "  3.59   0.876   0.876   0.876   0.903  \n",
      "  3.56   0.877   0.884   0.881   0.906  \n",
      "  3.13   0.873   0.851   0.862   0.893  \n",
      "  3.33   0.873   0.851   0.862   0.893  \n"
     ]
    }
   ],
   "source": [
    "## Some more code adapted from spaCy\n",
    "## this block ensures that we are only calling and training one pipeline 'textcat'\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    \n",
    "    ## spaCy trains a model with begin_training() using thinc.neural\n",
    "    optimizer = nlp.begin_training()\n",
    "    print(\"Training the model...\")\n",
    "    \n",
    "    ## redo as an f-string // Header column\n",
    "    print(f'{\"LOSS\":^8}{\"P\":^8}{\"R\":^8}{\"F\":^8}{\"A\":^8}')\n",
    "    \n",
    "    for i in range(n_iter):  ## computing our set number of iterations\n",
    "        losses = {}  ## creating a dictionary to store our losses\n",
    "        \n",
    "        ## using spaCy's minibatching to create a smaller batches of our data \n",
    "        ## for gradient descent optimization\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        \n",
    "        ## looping through the smaller batches\n",
    "        for batch in batches:\n",
    "            ## unzipping our batches and assigning the post text to 'texts' \n",
    "            ## and the 'cats' and 'bool_val' to 'annotations'\n",
    "            texts, annotations = zip(*batch)\n",
    "            \n",
    "            ## updating our model by using our 'optimizer' for gradient descent\n",
    "            ## setting a dropout rate of .2 to prevent overfitting our model\n",
    "            ## reevaluating our losses\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "        \n",
    "          ## encoding the parameters of thinc.neural on 'textcat'\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            \n",
    "            # evaluate on the dev data split off in load_data()\n",
    "            ## evaluate is the function from above\n",
    "            scores = evaluate(nlp.tokenizer, textcat, test_texts, test_cats)\n",
    "          \n",
    "        ## printing results (rounded, with centering and 8 spaces)\n",
    "        print(f'{round(losses[\"textcat\"],2):^8}'  \n",
    "              f'{round(scores[\"textcat_p\"],3):^8}' \n",
    "              f'{round(scores[\"textcat_r\"],3):^8}'\n",
    "              f'{round(scores[\"textcat_f\"],3):^8}'\n",
    "              f'{round(scores[\"textcat_a\"],3):^8}'\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">## Modeling via Sci-kit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, scorer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.608949\n",
       "1    0.391051\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reminder of our baseline\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating our train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=.3, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 14.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for CountVectorizer is: \n",
      "    0.7822\n",
      "    \n",
      "{'clf': MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True), 'clf__alpha': 0.5, 'vect__max_df': 0.9, 'vect__max_features': 500, 'vect__ngram_range': (1, 3), 'vect__stop_words': 'english'}\n",
      "\n",
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   43.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 13.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for Tf-IDF Vectorizer is: \n",
      "    0.7674\n",
      "    \n",
      "{'clf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=5,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'vect__max_df': 0.9, 'vect__max_features': 500, 'vect__ngram_range': (1, 3), 'vect__stop_words': 'english'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## creating a list of our two chosen vectorizers to iterate through in our grid search\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer()]\n",
    "\n",
    "## creating variables to accept tuning parameters\n",
    "max_feat = [300, 500]  \n",
    "ngram_range = [(1, 3), (1, 2)] \n",
    "stop_words = [None, 'english', custom_stop] \n",
    "max_df = [0.9, 0.8] \n",
    "\n",
    "## creating any empty results list to capture our cv_results_ at the end of each iteration\n",
    "results = []\n",
    "\n",
    "## looping through both vectorizers\n",
    "for vect in vectorizer:\n",
    "    \n",
    "    ## creating a pipeline for our vectorizer and classifier models\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', vect),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    ## generating our parameters for vectorizers\n",
    "    vect_params = {'vect__max_features': max_feat,\n",
    "                    'vect__stop_words': stop_words,\n",
    "                    'vect__ngram_range': ngram_range,\n",
    "                    'vect__max_df': max_df\n",
    "                  }\n",
    "    parameters = [\n",
    "        {\n",
    "            ## Logistic Regression\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (LogisticRegression(solver='liblinear'), ), ## setting our first classifier model\n",
    "            'clf__penalty': ('l1', 'l2'), #2\n",
    "            'clf__C': (.5, 1.0), #4 288*2\n",
    "        }, \n",
    "        {\n",
    "            ## Multinomial Bayes\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (MultinomialNB(), ),  ## setting our second classifier model\n",
    "            'clf__alpha': (.5, 1.0)  #2 72*2\n",
    "        },\n",
    "        {\n",
    "            ## SVC\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (SVC(gamma='scale', ), ),\n",
    "            'clf__kernel': ('rbf', 'poly') \n",
    "        },\n",
    "        {\n",
    "            ## RandomForestClassifier\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (RandomForestClassifier(n_estimators=50, min_samples_split=5), ),\n",
    "        },\n",
    "        {\n",
    "            ## putting together an ensemble model\n",
    "            'vect__max_features': max_feat,\n",
    "            'vect__stop_words': stop_words,\n",
    "            'vect__ngram_range': ngram_range,\n",
    "            'vect__max_df': max_df,\n",
    "            'clf': (VotingClassifier(estimators=[('lr', LogisticRegression()), \n",
    "                                                 ('rf', RandomForestClassifier()), \n",
    "                                                 ('mnb', MultinomialNB()), \n",
    "                                                 ('svc', SVC())],                                           \n",
    "                                            voting='hard'), )\n",
    "        }\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    ## performing our grid search with the inherited pipeline and parameters\n",
    "    grid_search = GridSearchCV(pipeline, \n",
    "                               parameters,\n",
    "                               cv=3,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=1,\n",
    "                               return_train_score=True\n",
    "                              )\n",
    "    \n",
    "    ## running an if statement to print the type of vectorizer used\n",
    "    if vect == vectorizer[0]:\n",
    "        vect_string = \"CountVectorizer\"\n",
    "    \n",
    "    else:\n",
    "        vect_string = \"Tf-IDF Vectorizer\"\n",
    "    \n",
    "    ## fitting our model and printing our best scores and parameters\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'''Best score for {vect_string} is: \n",
    "    {round(grid_search.best_score_, 4)}\n",
    "    ''')\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"\")\n",
    "    \n",
    "    ## appending our cv_results_ to the end of results\n",
    "    results.append(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## turning our results into a CV\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_clf__C</th>\n",
       "      <th>param_clf__penalty</th>\n",
       "      <th>param_vect__max_df</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>...</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.373203</td>\n",
       "      <td>0.078691</td>\n",
       "      <td>0.369766</td>\n",
       "      <td>0.041348</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.704735</td>\n",
       "      <td>0.689527</td>\n",
       "      <td>0.014818</td>\n",
       "      <td>215</td>\n",
       "      <td>0.719054</td>\n",
       "      <td>0.714882</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.721034</td>\n",
       "      <td>0.005997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.164617</td>\n",
       "      <td>0.076181</td>\n",
       "      <td>0.227521</td>\n",
       "      <td>0.027277</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.724234</td>\n",
       "      <td>0.719184</td>\n",
       "      <td>0.007685</td>\n",
       "      <td>158</td>\n",
       "      <td>0.734353</td>\n",
       "      <td>0.739917</td>\n",
       "      <td>0.761111</td>\n",
       "      <td>0.745127</td>\n",
       "      <td>0.011528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.729589</td>\n",
       "      <td>0.108484</td>\n",
       "      <td>0.385576</td>\n",
       "      <td>0.066235</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.704735</td>\n",
       "      <td>0.690454</td>\n",
       "      <td>0.015179</td>\n",
       "      <td>214</td>\n",
       "      <td>0.720445</td>\n",
       "      <td>0.714882</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.721035</td>\n",
       "      <td>0.005281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.024234</td>\n",
       "      <td>0.063128</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.020190</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.701950</td>\n",
       "      <td>0.688601</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>217</td>\n",
       "      <td>0.719054</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.720107</td>\n",
       "      <td>0.007007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.740609</td>\n",
       "      <td>0.031643</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.033541</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.724234</td>\n",
       "      <td>0.719184</td>\n",
       "      <td>0.007685</td>\n",
       "      <td>158</td>\n",
       "      <td>0.734353</td>\n",
       "      <td>0.738526</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.745126</td>\n",
       "      <td>0.012403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.354643</td>\n",
       "      <td>0.232952</td>\n",
       "      <td>0.278203</td>\n",
       "      <td>0.049495</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.704735</td>\n",
       "      <td>0.689527</td>\n",
       "      <td>0.014818</td>\n",
       "      <td>215</td>\n",
       "      <td>0.719054</td>\n",
       "      <td>0.713491</td>\n",
       "      <td>0.730556</td>\n",
       "      <td>0.721034</td>\n",
       "      <td>0.007106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.469879</td>\n",
       "      <td>1.016552</td>\n",
       "      <td>1.125345</td>\n",
       "      <td>0.443995</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.701950</td>\n",
       "      <td>0.683040</td>\n",
       "      <td>0.014508</td>\n",
       "      <td>231</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.699583</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>0.710376</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.909729</td>\n",
       "      <td>0.736120</td>\n",
       "      <td>0.681134</td>\n",
       "      <td>0.532587</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702778</td>\n",
       "      <td>0.718663</td>\n",
       "      <td>0.708990</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>182</td>\n",
       "      <td>0.728790</td>\n",
       "      <td>0.720445</td>\n",
       "      <td>0.751389</td>\n",
       "      <td>0.733541</td>\n",
       "      <td>0.013072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.704087</td>\n",
       "      <td>0.991593</td>\n",
       "      <td>0.755768</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.699164</td>\n",
       "      <td>0.683040</td>\n",
       "      <td>0.013266</td>\n",
       "      <td>231</td>\n",
       "      <td>0.710709</td>\n",
       "      <td>0.698192</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>0.709449</td>\n",
       "      <td>0.008722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.030254</td>\n",
       "      <td>0.490545</td>\n",
       "      <td>0.438581</td>\n",
       "      <td>0.083735</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.699164</td>\n",
       "      <td>0.681186</td>\n",
       "      <td>0.013482</td>\n",
       "      <td>234</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.699583</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>0.710376</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.696661</td>\n",
       "      <td>0.279868</td>\n",
       "      <td>0.184989</td>\n",
       "      <td>0.029428</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702778</td>\n",
       "      <td>0.718663</td>\n",
       "      <td>0.708990</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>182</td>\n",
       "      <td>0.727399</td>\n",
       "      <td>0.720445</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.732615</td>\n",
       "      <td>0.012617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.940442</td>\n",
       "      <td>0.452313</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.045269</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.699164</td>\n",
       "      <td>0.681186</td>\n",
       "      <td>0.013482</td>\n",
       "      <td>234</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.698192</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>0.709912</td>\n",
       "      <td>0.008813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.042898</td>\n",
       "      <td>0.041512</td>\n",
       "      <td>0.501629</td>\n",
       "      <td>0.129180</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.713092</td>\n",
       "      <td>0.693234</td>\n",
       "      <td>0.014738</td>\n",
       "      <td>208</td>\n",
       "      <td>0.726008</td>\n",
       "      <td>0.719054</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.728910</td>\n",
       "      <td>0.009457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.082653</td>\n",
       "      <td>0.075055</td>\n",
       "      <td>0.281324</td>\n",
       "      <td>0.053157</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.724234</td>\n",
       "      <td>0.719184</td>\n",
       "      <td>0.007685</td>\n",
       "      <td>158</td>\n",
       "      <td>0.734353</td>\n",
       "      <td>0.739917</td>\n",
       "      <td>0.761111</td>\n",
       "      <td>0.745127</td>\n",
       "      <td>0.011528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.657002</td>\n",
       "      <td>0.241044</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.099046</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.713092</td>\n",
       "      <td>0.695088</td>\n",
       "      <td>0.014421</td>\n",
       "      <td>206</td>\n",
       "      <td>0.726008</td>\n",
       "      <td>0.720445</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.729836</td>\n",
       "      <td>0.009619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.083127</td>\n",
       "      <td>0.129385</td>\n",
       "      <td>0.321105</td>\n",
       "      <td>0.049104</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.713092</td>\n",
       "      <td>0.695088</td>\n",
       "      <td>0.014421</td>\n",
       "      <td>206</td>\n",
       "      <td>0.724618</td>\n",
       "      <td>0.716273</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.727519</td>\n",
       "      <td>0.010568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.046239</td>\n",
       "      <td>0.199161</td>\n",
       "      <td>0.184858</td>\n",
       "      <td>0.042855</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.724234</td>\n",
       "      <td>0.719184</td>\n",
       "      <td>0.007685</td>\n",
       "      <td>158</td>\n",
       "      <td>0.734353</td>\n",
       "      <td>0.738526</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.745126</td>\n",
       "      <td>0.012403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.301143</td>\n",
       "      <td>0.268067</td>\n",
       "      <td>0.266958</td>\n",
       "      <td>0.030852</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.713092</td>\n",
       "      <td>0.696015</td>\n",
       "      <td>0.014439</td>\n",
       "      <td>205</td>\n",
       "      <td>0.726008</td>\n",
       "      <td>0.719054</td>\n",
       "      <td>0.741667</td>\n",
       "      <td>0.728910</td>\n",
       "      <td>0.009457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.638599</td>\n",
       "      <td>0.448291</td>\n",
       "      <td>0.591621</td>\n",
       "      <td>0.135275</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.707521</td>\n",
       "      <td>0.687674</td>\n",
       "      <td>0.014731</td>\n",
       "      <td>218</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.709318</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.718251</td>\n",
       "      <td>0.010725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.175142</td>\n",
       "      <td>0.090326</td>\n",
       "      <td>0.276586</td>\n",
       "      <td>0.011092</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702778</td>\n",
       "      <td>0.718663</td>\n",
       "      <td>0.708990</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>182</td>\n",
       "      <td>0.728790</td>\n",
       "      <td>0.720445</td>\n",
       "      <td>0.751389</td>\n",
       "      <td>0.733541</td>\n",
       "      <td>0.013072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.636528</td>\n",
       "      <td>0.469246</td>\n",
       "      <td>0.574613</td>\n",
       "      <td>0.162417</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.707521</td>\n",
       "      <td>0.687674</td>\n",
       "      <td>0.014731</td>\n",
       "      <td>218</td>\n",
       "      <td>0.713491</td>\n",
       "      <td>0.709318</td>\n",
       "      <td>0.731944</td>\n",
       "      <td>0.718251</td>\n",
       "      <td>0.009831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.328291</td>\n",
       "      <td>0.326374</td>\n",
       "      <td>0.538552</td>\n",
       "      <td>0.158213</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.707521</td>\n",
       "      <td>0.687674</td>\n",
       "      <td>0.014731</td>\n",
       "      <td>218</td>\n",
       "      <td>0.713491</td>\n",
       "      <td>0.709318</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.719640</td>\n",
       "      <td>0.011771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.708833</td>\n",
       "      <td>0.071052</td>\n",
       "      <td>0.181456</td>\n",
       "      <td>0.045820</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702778</td>\n",
       "      <td>0.718663</td>\n",
       "      <td>0.708990</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>182</td>\n",
       "      <td>0.727399</td>\n",
       "      <td>0.720445</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.732615</td>\n",
       "      <td>0.012617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.316360</td>\n",
       "      <td>0.248432</td>\n",
       "      <td>0.243947</td>\n",
       "      <td>0.034134</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.707521</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.015059</td>\n",
       "      <td>223</td>\n",
       "      <td>0.714882</td>\n",
       "      <td>0.709318</td>\n",
       "      <td>0.734722</td>\n",
       "      <td>0.719641</td>\n",
       "      <td>0.010903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.226000</td>\n",
       "      <td>0.075495</td>\n",
       "      <td>0.399819</td>\n",
       "      <td>0.015116</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702778</td>\n",
       "      <td>0.729805</td>\n",
       "      <td>0.721965</td>\n",
       "      <td>0.013653</td>\n",
       "      <td>148</td>\n",
       "      <td>0.783032</td>\n",
       "      <td>0.792768</td>\n",
       "      <td>0.784722</td>\n",
       "      <td>0.786841</td>\n",
       "      <td>0.004248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.479578</td>\n",
       "      <td>0.147372</td>\n",
       "      <td>0.267982</td>\n",
       "      <td>0.053521</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730556</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.740500</td>\n",
       "      <td>0.012646</td>\n",
       "      <td>95</td>\n",
       "      <td>0.815021</td>\n",
       "      <td>0.815021</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.815569</td>\n",
       "      <td>0.000776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.553993</td>\n",
       "      <td>0.187165</td>\n",
       "      <td>0.401203</td>\n",
       "      <td>0.045333</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.718663</td>\n",
       "      <td>0.721965</td>\n",
       "      <td>0.010474</td>\n",
       "      <td>148</td>\n",
       "      <td>0.787204</td>\n",
       "      <td>0.796940</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.789159</td>\n",
       "      <td>0.005724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.110014</td>\n",
       "      <td>0.040841</td>\n",
       "      <td>0.252796</td>\n",
       "      <td>0.032029</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705556</td>\n",
       "      <td>0.727019</td>\n",
       "      <td>0.725672</td>\n",
       "      <td>0.015912</td>\n",
       "      <td>143</td>\n",
       "      <td>0.784423</td>\n",
       "      <td>0.794159</td>\n",
       "      <td>0.784722</td>\n",
       "      <td>0.787768</td>\n",
       "      <td>0.004521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.710585</td>\n",
       "      <td>0.027161</td>\n",
       "      <td>0.131823</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730556</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.739574</td>\n",
       "      <td>0.011339</td>\n",
       "      <td>98</td>\n",
       "      <td>0.813630</td>\n",
       "      <td>0.815021</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.816032</td>\n",
       "      <td>0.002479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.275996</td>\n",
       "      <td>0.209648</td>\n",
       "      <td>0.245091</td>\n",
       "      <td>0.034969</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.721448</td>\n",
       "      <td>0.719184</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>158</td>\n",
       "      <td>0.784423</td>\n",
       "      <td>0.796940</td>\n",
       "      <td>0.784722</td>\n",
       "      <td>0.788695</td>\n",
       "      <td>0.005831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>3.991225</td>\n",
       "      <td>0.299356</td>\n",
       "      <td>0.603854</td>\n",
       "      <td>0.125741</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.729805</td>\n",
       "      <td>0.721965</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994437</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.996757</td>\n",
       "      <td>0.002363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2.357903</td>\n",
       "      <td>0.597689</td>\n",
       "      <td>0.721938</td>\n",
       "      <td>0.414403</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.735376</td>\n",
       "      <td>0.755329</td>\n",
       "      <td>0.015188</td>\n",
       "      <td>29</td>\n",
       "      <td>0.995828</td>\n",
       "      <td>0.994437</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.994440</td>\n",
       "      <td>0.001132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>5.287769</td>\n",
       "      <td>0.598880</td>\n",
       "      <td>0.597443</td>\n",
       "      <td>0.015733</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697222</td>\n",
       "      <td>0.718663</td>\n",
       "      <td>0.721038</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>151</td>\n",
       "      <td>0.998609</td>\n",
       "      <td>0.994437</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>0.996293</td>\n",
       "      <td>0.001734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2.218427</td>\n",
       "      <td>0.129756</td>\n",
       "      <td>0.377855</td>\n",
       "      <td>0.057206</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.710306</td>\n",
       "      <td>0.715477</td>\n",
       "      <td>0.021863</td>\n",
       "      <td>172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994437</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>0.996294</td>\n",
       "      <td>0.002621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1.485906</td>\n",
       "      <td>0.050153</td>\n",
       "      <td>0.270736</td>\n",
       "      <td>0.075265</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.760446</td>\n",
       "      <td>0.754402</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>32</td>\n",
       "      <td>0.997218</td>\n",
       "      <td>0.994437</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.994904</td>\n",
       "      <td>0.001731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>3.407893</td>\n",
       "      <td>0.882642</td>\n",
       "      <td>0.681929</td>\n",
       "      <td>0.235767</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686111</td>\n",
       "      <td>0.688022</td>\n",
       "      <td>0.708063</td>\n",
       "      <td>0.029685</td>\n",
       "      <td>186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997218</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>0.997221</td>\n",
       "      <td>0.002268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>4.819316</td>\n",
       "      <td>0.476665</td>\n",
       "      <td>0.838554</td>\n",
       "      <td>0.092036</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.699164</td>\n",
       "      <td>0.683967</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>228</td>\n",
       "      <td>0.764951</td>\n",
       "      <td>0.763561</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.762282</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2.678625</td>\n",
       "      <td>0.347356</td>\n",
       "      <td>0.572089</td>\n",
       "      <td>0.107652</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.713092</td>\n",
       "      <td>0.709917</td>\n",
       "      <td>0.011565</td>\n",
       "      <td>180</td>\n",
       "      <td>0.827538</td>\n",
       "      <td>0.833102</td>\n",
       "      <td>0.843056</td>\n",
       "      <td>0.834565</td>\n",
       "      <td>0.006419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>5.079005</td>\n",
       "      <td>0.255440</td>\n",
       "      <td>0.805930</td>\n",
       "      <td>0.035197</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.690808</td>\n",
       "      <td>0.683967</td>\n",
       "      <td>0.010282</td>\n",
       "      <td>228</td>\n",
       "      <td>0.769124</td>\n",
       "      <td>0.767733</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.766452</td>\n",
       "      <td>0.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2.004655</td>\n",
       "      <td>0.085097</td>\n",
       "      <td>0.837995</td>\n",
       "      <td>0.173188</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.685237</td>\n",
       "      <td>0.682113</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>233</td>\n",
       "      <td>0.764951</td>\n",
       "      <td>0.764951</td>\n",
       "      <td>0.759722</td>\n",
       "      <td>0.763208</td>\n",
       "      <td>0.002465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1.600050</td>\n",
       "      <td>0.042104</td>\n",
       "      <td>0.472926</td>\n",
       "      <td>0.057868</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.729805</td>\n",
       "      <td>0.721038</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>151</td>\n",
       "      <td>0.828929</td>\n",
       "      <td>0.833102</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.836418</td>\n",
       "      <td>0.007828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2.590960</td>\n",
       "      <td>0.418256</td>\n",
       "      <td>0.618058</td>\n",
       "      <td>0.063861</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.671309</td>\n",
       "      <td>0.673772</td>\n",
       "      <td>0.015979</td>\n",
       "      <td>240</td>\n",
       "      <td>0.770515</td>\n",
       "      <td>0.766342</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.767841</td>\n",
       "      <td>0.001895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>4.374735</td>\n",
       "      <td>0.341199</td>\n",
       "      <td>0.937203</td>\n",
       "      <td>0.168975</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.685237</td>\n",
       "      <td>0.679333</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>238</td>\n",
       "      <td>0.789986</td>\n",
       "      <td>0.784423</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.784525</td>\n",
       "      <td>0.004418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2.843553</td>\n",
       "      <td>0.219490</td>\n",
       "      <td>0.644750</td>\n",
       "      <td>0.252388</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.721448</td>\n",
       "      <td>0.717331</td>\n",
       "      <td>0.008457</td>\n",
       "      <td>166</td>\n",
       "      <td>0.845619</td>\n",
       "      <td>0.847010</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.848932</td>\n",
       "      <td>0.003745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>5.092791</td>\n",
       "      <td>0.406627</td>\n",
       "      <td>0.925251</td>\n",
       "      <td>0.175044</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.688022</td>\n",
       "      <td>0.684893</td>\n",
       "      <td>0.009341</td>\n",
       "      <td>225</td>\n",
       "      <td>0.792768</td>\n",
       "      <td>0.785814</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.785453</td>\n",
       "      <td>0.006125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2.993511</td>\n",
       "      <td>0.162401</td>\n",
       "      <td>0.630201</td>\n",
       "      <td>0.069169</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.696379</td>\n",
       "      <td>0.681186</td>\n",
       "      <td>0.018238</td>\n",
       "      <td>234</td>\n",
       "      <td>0.789986</td>\n",
       "      <td>0.785814</td>\n",
       "      <td>0.779167</td>\n",
       "      <td>0.784989</td>\n",
       "      <td>0.004455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1.447532</td>\n",
       "      <td>0.409208</td>\n",
       "      <td>0.349527</td>\n",
       "      <td>0.128096</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.727019</td>\n",
       "      <td>0.716404</td>\n",
       "      <td>0.011771</td>\n",
       "      <td>170</td>\n",
       "      <td>0.844228</td>\n",
       "      <td>0.844228</td>\n",
       "      <td>0.856944</td>\n",
       "      <td>0.848467</td>\n",
       "      <td>0.005995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2.900140</td>\n",
       "      <td>0.208663</td>\n",
       "      <td>0.702300</td>\n",
       "      <td>0.043523</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663889</td>\n",
       "      <td>0.699164</td>\n",
       "      <td>0.684893</td>\n",
       "      <td>0.015175</td>\n",
       "      <td>225</td>\n",
       "      <td>0.792768</td>\n",
       "      <td>0.791377</td>\n",
       "      <td>0.773611</td>\n",
       "      <td>0.785919</td>\n",
       "      <td>0.008721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>4.164300</td>\n",
       "      <td>0.217321</td>\n",
       "      <td>0.689130</td>\n",
       "      <td>0.021527</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.699164</td>\n",
       "      <td>0.687674</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>218</td>\n",
       "      <td>0.778860</td>\n",
       "      <td>0.777469</td>\n",
       "      <td>0.776389</td>\n",
       "      <td>0.777572</td>\n",
       "      <td>0.001011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2.531729</td>\n",
       "      <td>0.160352</td>\n",
       "      <td>0.426849</td>\n",
       "      <td>0.118977</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705556</td>\n",
       "      <td>0.740947</td>\n",
       "      <td>0.721038</td>\n",
       "      <td>0.014773</td>\n",
       "      <td>151</td>\n",
       "      <td>0.827538</td>\n",
       "      <td>0.834492</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.835492</td>\n",
       "      <td>0.006938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>5.114450</td>\n",
       "      <td>0.439929</td>\n",
       "      <td>0.937774</td>\n",
       "      <td>0.229941</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.696379</td>\n",
       "      <td>0.683967</td>\n",
       "      <td>0.011098</td>\n",
       "      <td>228</td>\n",
       "      <td>0.783032</td>\n",
       "      <td>0.778860</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.781742</td>\n",
       "      <td>0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>2.469381</td>\n",
       "      <td>0.354824</td>\n",
       "      <td>0.537635</td>\n",
       "      <td>0.070044</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.685237</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.012526</td>\n",
       "      <td>223</td>\n",
       "      <td>0.773296</td>\n",
       "      <td>0.780250</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.776182</td>\n",
       "      <td>0.002960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1.376532</td>\n",
       "      <td>0.086879</td>\n",
       "      <td>0.342108</td>\n",
       "      <td>0.029176</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713889</td>\n",
       "      <td>0.718663</td>\n",
       "      <td>0.719184</td>\n",
       "      <td>0.004553</td>\n",
       "      <td>158</td>\n",
       "      <td>0.830320</td>\n",
       "      <td>0.833102</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.835955</td>\n",
       "      <td>0.006109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2.689863</td>\n",
       "      <td>0.334089</td>\n",
       "      <td>0.759689</td>\n",
       "      <td>0.196371</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>300</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.699164</td>\n",
       "      <td>0.687674</td>\n",
       "      <td>0.014886</td>\n",
       "      <td>218</td>\n",
       "      <td>0.778860</td>\n",
       "      <td>0.783032</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.781742</td>\n",
       "      <td>0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>4.485170</td>\n",
       "      <td>0.228051</td>\n",
       "      <td>0.825697</td>\n",
       "      <td>0.224565</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.690808</td>\n",
       "      <td>0.684893</td>\n",
       "      <td>0.011032</td>\n",
       "      <td>225</td>\n",
       "      <td>0.798331</td>\n",
       "      <td>0.791377</td>\n",
       "      <td>0.790278</td>\n",
       "      <td>0.793329</td>\n",
       "      <td>0.003566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2.936999</td>\n",
       "      <td>0.388277</td>\n",
       "      <td>0.420936</td>\n",
       "      <td>0.069526</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719444</td>\n",
       "      <td>0.746518</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.018019</td>\n",
       "      <td>146</td>\n",
       "      <td>0.845619</td>\n",
       "      <td>0.847010</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.847543</td>\n",
       "      <td>0.001828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>5.374208</td>\n",
       "      <td>0.236892</td>\n",
       "      <td>1.016555</td>\n",
       "      <td>0.204019</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.704735</td>\n",
       "      <td>0.691381</td>\n",
       "      <td>0.011005</td>\n",
       "      <td>213</td>\n",
       "      <td>0.803894</td>\n",
       "      <td>0.794159</td>\n",
       "      <td>0.798611</td>\n",
       "      <td>0.798888</td>\n",
       "      <td>0.003979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2.397690</td>\n",
       "      <td>0.032069</td>\n",
       "      <td>0.578282</td>\n",
       "      <td>0.184381</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.688022</td>\n",
       "      <td>0.679333</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>237</td>\n",
       "      <td>0.798331</td>\n",
       "      <td>0.792768</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.002330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1.602526</td>\n",
       "      <td>0.174750</td>\n",
       "      <td>0.454591</td>\n",
       "      <td>0.075519</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705556</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.715477</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>172</td>\n",
       "      <td>0.848401</td>\n",
       "      <td>0.848401</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.850323</td>\n",
       "      <td>0.002718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1.863748</td>\n",
       "      <td>0.284503</td>\n",
       "      <td>0.336527</td>\n",
       "      <td>0.069208</td>\n",
       "      <td>VotingClassifier(estimators=[('lr',\\n         ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669444</td>\n",
       "      <td>0.693593</td>\n",
       "      <td>0.678406</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>239</td>\n",
       "      <td>0.801113</td>\n",
       "      <td>0.802503</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.799354</td>\n",
       "      <td>0.003517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         2.373203      0.078691         0.369766        0.041348   \n",
       "1         1.164617      0.076181         0.227521        0.027277   \n",
       "2         2.729589      0.108484         0.385576        0.066235   \n",
       "3         1.024234      0.063128         0.223400        0.020190   \n",
       "4         0.740609      0.031643         0.164100        0.033541   \n",
       "5         1.354643      0.232952         0.278203        0.049495   \n",
       "6         3.469879      1.016552         1.125345        0.443995   \n",
       "7         3.909729      0.736120         0.681134        0.532587   \n",
       "8         3.704087      0.991593         0.755768        0.225352   \n",
       "9         2.030254      0.490545         0.438581        0.083735   \n",
       "10        1.696661      0.279868         0.184989        0.029428   \n",
       "11        1.940442      0.452313         0.274510        0.045269   \n",
       "12        3.042898      0.041512         0.501629        0.129180   \n",
       "13        2.082653      0.075055         0.281324        0.053157   \n",
       "14        2.657002      0.241044         0.472000        0.099046   \n",
       "15        1.083127      0.129385         0.321105        0.049104   \n",
       "16        1.046239      0.199161         0.184858        0.042855   \n",
       "17        1.301143      0.268067         0.266958        0.030852   \n",
       "18        2.638599      0.448291         0.591621        0.135275   \n",
       "19        2.175142      0.090326         0.276586        0.011092   \n",
       "20        4.636528      0.469246         0.574613        0.162417   \n",
       "21        2.328291      0.326374         0.538552        0.158213   \n",
       "22        1.708833      0.071052         0.181456        0.045820   \n",
       "23        1.316360      0.248432         0.243947        0.034134   \n",
       "24        2.226000      0.075495         0.399819        0.015116   \n",
       "25        1.479578      0.147372         0.267982        0.053521   \n",
       "26        2.553993      0.187165         0.401203        0.045333   \n",
       "27        1.110014      0.040841         0.252796        0.032029   \n",
       "28        0.710585      0.027161         0.131823        0.022733   \n",
       "29        1.275996      0.209648         0.245091        0.034969   \n",
       "..             ...           ...              ...             ...   \n",
       "210       3.991225      0.299356         0.603854        0.125741   \n",
       "211       2.357903      0.597689         0.721938        0.414403   \n",
       "212       5.287769      0.598880         0.597443        0.015733   \n",
       "213       2.218427      0.129756         0.377855        0.057206   \n",
       "214       1.485906      0.050153         0.270736        0.075265   \n",
       "215       3.407893      0.882642         0.681929        0.235767   \n",
       "216       4.819316      0.476665         0.838554        0.092036   \n",
       "217       2.678625      0.347356         0.572089        0.107652   \n",
       "218       5.079005      0.255440         0.805930        0.035197   \n",
       "219       2.004655      0.085097         0.837995        0.173188   \n",
       "220       1.600050      0.042104         0.472926        0.057868   \n",
       "221       2.590960      0.418256         0.618058        0.063861   \n",
       "222       4.374735      0.341199         0.937203        0.168975   \n",
       "223       2.843553      0.219490         0.644750        0.252388   \n",
       "224       5.092791      0.406627         0.925251        0.175044   \n",
       "225       2.993511      0.162401         0.630201        0.069169   \n",
       "226       1.447532      0.409208         0.349527        0.128096   \n",
       "227       2.900140      0.208663         0.702300        0.043523   \n",
       "228       4.164300      0.217321         0.689130        0.021527   \n",
       "229       2.531729      0.160352         0.426849        0.118977   \n",
       "230       5.114450      0.439929         0.937774        0.229941   \n",
       "231       2.469381      0.354824         0.537635        0.070044   \n",
       "232       1.376532      0.086879         0.342108        0.029176   \n",
       "233       2.689863      0.334089         0.759689        0.196371   \n",
       "234       4.485170      0.228051         0.825697        0.224565   \n",
       "235       2.936999      0.388277         0.420936        0.069526   \n",
       "236       5.374208      0.236892         1.016555        0.204019   \n",
       "237       2.397690      0.032069         0.578282        0.184381   \n",
       "238       1.602526      0.174750         0.454591        0.075519   \n",
       "239       1.863748      0.284503         0.336527        0.069208   \n",
       "\n",
       "                                             param_clf param_clf__C  \\\n",
       "0    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "1    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "2    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "3    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "4    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "5    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "6    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "7    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "8    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "9    LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "10   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "11   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "12   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "13   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "14   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "15   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "16   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "17   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "18   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "19   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "20   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "21   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "22   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "23   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "24   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "25   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "26   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "27   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "28   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "29   LogisticRegression(C=1.0, class_weight=None, d...          0.5   \n",
       "..                                                 ...          ...   \n",
       "210  (DecisionTreeClassifier(class_weight=None, cri...          NaN   \n",
       "211  (DecisionTreeClassifier(class_weight=None, cri...          NaN   \n",
       "212  (DecisionTreeClassifier(class_weight=None, cri...          NaN   \n",
       "213  (DecisionTreeClassifier(class_weight=None, cri...          NaN   \n",
       "214  (DecisionTreeClassifier(class_weight=None, cri...          NaN   \n",
       "215  (DecisionTreeClassifier(class_weight=None, cri...          NaN   \n",
       "216  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "217  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "218  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "219  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "220  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "221  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "222  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "223  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "224  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "225  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "226  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "227  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "228  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "229  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "230  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "231  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "232  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "233  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "234  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "235  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "236  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "237  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "238  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "239  VotingClassifier(estimators=[('lr',\\n         ...          NaN   \n",
       "\n",
       "    param_clf__penalty param_vect__max_df param_vect__max_features  \\\n",
       "0                   l1                0.9                      300   \n",
       "1                   l1                0.9                      300   \n",
       "2                   l1                0.9                      300   \n",
       "3                   l1                0.9                      300   \n",
       "4                   l1                0.9                      300   \n",
       "5                   l1                0.9                      300   \n",
       "6                   l1                0.9                      500   \n",
       "7                   l1                0.9                      500   \n",
       "8                   l1                0.9                      500   \n",
       "9                   l1                0.9                      500   \n",
       "10                  l1                0.9                      500   \n",
       "11                  l1                0.9                      500   \n",
       "12                  l1                0.8                      300   \n",
       "13                  l1                0.8                      300   \n",
       "14                  l1                0.8                      300   \n",
       "15                  l1                0.8                      300   \n",
       "16                  l1                0.8                      300   \n",
       "17                  l1                0.8                      300   \n",
       "18                  l1                0.8                      500   \n",
       "19                  l1                0.8                      500   \n",
       "20                  l1                0.8                      500   \n",
       "21                  l1                0.8                      500   \n",
       "22                  l1                0.8                      500   \n",
       "23                  l1                0.8                      500   \n",
       "24                  l2                0.9                      300   \n",
       "25                  l2                0.9                      300   \n",
       "26                  l2                0.9                      300   \n",
       "27                  l2                0.9                      300   \n",
       "28                  l2                0.9                      300   \n",
       "29                  l2                0.9                      300   \n",
       "..                 ...                ...                      ...   \n",
       "210                NaN                0.8                      500   \n",
       "211                NaN                0.8                      500   \n",
       "212                NaN                0.8                      500   \n",
       "213                NaN                0.8                      500   \n",
       "214                NaN                0.8                      500   \n",
       "215                NaN                0.8                      500   \n",
       "216                NaN                0.9                      300   \n",
       "217                NaN                0.9                      300   \n",
       "218                NaN                0.9                      300   \n",
       "219                NaN                0.9                      300   \n",
       "220                NaN                0.9                      300   \n",
       "221                NaN                0.9                      300   \n",
       "222                NaN                0.9                      500   \n",
       "223                NaN                0.9                      500   \n",
       "224                NaN                0.9                      500   \n",
       "225                NaN                0.9                      500   \n",
       "226                NaN                0.9                      500   \n",
       "227                NaN                0.9                      500   \n",
       "228                NaN                0.8                      300   \n",
       "229                NaN                0.8                      300   \n",
       "230                NaN                0.8                      300   \n",
       "231                NaN                0.8                      300   \n",
       "232                NaN                0.8                      300   \n",
       "233                NaN                0.8                      300   \n",
       "234                NaN                0.8                      500   \n",
       "235                NaN                0.8                      500   \n",
       "236                NaN                0.8                      500   \n",
       "237                NaN                0.8                      500   \n",
       "238                NaN                0.8                      500   \n",
       "239                NaN                0.8                      500   \n",
       "\n",
       "    param_vect__ngram_range  ... split1_test_score split2_test_score  \\\n",
       "0                    (1, 3)  ...          0.669444          0.704735   \n",
       "1                    (1, 3)  ...          0.708333          0.724234   \n",
       "2                    (1, 3)  ...          0.669444          0.704735   \n",
       "3                    (1, 2)  ...          0.669444          0.701950   \n",
       "4                    (1, 2)  ...          0.708333          0.724234   \n",
       "5                    (1, 2)  ...          0.669444          0.704735   \n",
       "6                    (1, 3)  ...          0.666667          0.701950   \n",
       "7                    (1, 3)  ...          0.702778          0.718663   \n",
       "8                    (1, 3)  ...          0.666667          0.699164   \n",
       "9                    (1, 2)  ...          0.666667          0.699164   \n",
       "10                   (1, 2)  ...          0.702778          0.718663   \n",
       "11                   (1, 2)  ...          0.666667          0.699164   \n",
       "12                   (1, 3)  ...          0.677778          0.713092   \n",
       "13                   (1, 3)  ...          0.708333          0.724234   \n",
       "14                   (1, 3)  ...          0.677778          0.713092   \n",
       "15                   (1, 2)  ...          0.677778          0.713092   \n",
       "16                   (1, 2)  ...          0.708333          0.724234   \n",
       "17                   (1, 2)  ...          0.677778          0.713092   \n",
       "18                   (1, 3)  ...          0.672222          0.707521   \n",
       "19                   (1, 3)  ...          0.702778          0.718663   \n",
       "20                   (1, 3)  ...          0.672222          0.707521   \n",
       "21                   (1, 2)  ...          0.672222          0.707521   \n",
       "22                   (1, 2)  ...          0.702778          0.718663   \n",
       "23                   (1, 2)  ...          0.672222          0.707521   \n",
       "24                   (1, 3)  ...          0.702778          0.729805   \n",
       "25                   (1, 3)  ...          0.730556          0.732591   \n",
       "26                   (1, 3)  ...          0.711111          0.718663   \n",
       "27                   (1, 2)  ...          0.705556          0.727019   \n",
       "28                   (1, 2)  ...          0.730556          0.732591   \n",
       "29                   (1, 2)  ...          0.711111          0.721448   \n",
       "..                      ...  ...               ...               ...   \n",
       "210                  (1, 3)  ...          0.691667          0.729805   \n",
       "211                  (1, 3)  ...          0.758333          0.735376   \n",
       "212                  (1, 3)  ...          0.697222          0.718663   \n",
       "213                  (1, 2)  ...          0.691667          0.710306   \n",
       "214                  (1, 2)  ...          0.744444          0.760446   \n",
       "215                  (1, 2)  ...          0.686111          0.688022   \n",
       "216                  (1, 3)  ...          0.675000          0.699164   \n",
       "217                  (1, 3)  ...          0.694444          0.713092   \n",
       "218                  (1, 3)  ...          0.669444          0.690808   \n",
       "219                  (1, 2)  ...          0.675000          0.685237   \n",
       "220                  (1, 2)  ...          0.708333          0.729805   \n",
       "221                  (1, 2)  ...          0.655556          0.671309   \n",
       "222                  (1, 3)  ...          0.669444          0.685237   \n",
       "223                  (1, 3)  ...          0.725000          0.721448   \n",
       "224                  (1, 3)  ...          0.672222          0.688022   \n",
       "225                  (1, 2)  ...          0.655556          0.696379   \n",
       "226                  (1, 2)  ...          0.722222          0.727019   \n",
       "227                  (1, 2)  ...          0.663889          0.699164   \n",
       "228                  (1, 3)  ...          0.683333          0.699164   \n",
       "229                  (1, 3)  ...          0.705556          0.740947   \n",
       "230                  (1, 3)  ...          0.669444          0.696379   \n",
       "231                  (1, 2)  ...          0.672222          0.685237   \n",
       "232                  (1, 2)  ...          0.713889          0.718663   \n",
       "233                  (1, 2)  ...          0.666667          0.699164   \n",
       "234                  (1, 3)  ...          0.669444          0.690808   \n",
       "235                  (1, 3)  ...          0.719444          0.746518   \n",
       "236                  (1, 3)  ...          0.677778          0.704735   \n",
       "237                  (1, 2)  ...          0.675000          0.688022   \n",
       "238                  (1, 2)  ...          0.705556          0.732591   \n",
       "239                  (1, 2)  ...          0.669444          0.693593   \n",
       "\n",
       "    mean_test_score std_test_score  rank_test_score  split0_train_score  \\\n",
       "0          0.689527       0.014818              215            0.719054   \n",
       "1          0.719184       0.007685              158            0.734353   \n",
       "2          0.690454       0.015179              214            0.720445   \n",
       "3          0.688601       0.013897              217            0.719054   \n",
       "4          0.719184       0.007685              158            0.734353   \n",
       "5          0.689527       0.014818              215            0.719054   \n",
       "6          0.683040       0.014508              231            0.712100   \n",
       "7          0.708990       0.006924              182            0.728790   \n",
       "8          0.683040       0.013266              231            0.710709   \n",
       "9          0.681186       0.013482              234            0.712100   \n",
       "10         0.708990       0.006924              182            0.727399   \n",
       "11         0.681186       0.013482              234            0.712100   \n",
       "12         0.693234       0.014738              208            0.726008   \n",
       "13         0.719184       0.007685              158            0.734353   \n",
       "14         0.695088       0.014421              206            0.726008   \n",
       "15         0.695088       0.014421              206            0.724618   \n",
       "16         0.719184       0.007685              158            0.734353   \n",
       "17         0.696015       0.014439              205            0.726008   \n",
       "18         0.687674       0.014731              218            0.712100   \n",
       "19         0.708990       0.006924              182            0.728790   \n",
       "20         0.687674       0.014731              218            0.713491   \n",
       "21         0.687674       0.014731              218            0.713491   \n",
       "22         0.708990       0.006924              182            0.727399   \n",
       "23         0.686747       0.015059              223            0.714882   \n",
       "24         0.721965       0.013653              148            0.783032   \n",
       "25         0.740500       0.012646               95            0.815021   \n",
       "26         0.721965       0.010474              148            0.787204   \n",
       "27         0.725672       0.015912              143            0.784423   \n",
       "28         0.739574       0.011339               98            0.813630   \n",
       "29         0.719184       0.005894              158            0.784423   \n",
       "..              ...            ...              ...                 ...   \n",
       "210        0.721965       0.022256              148            1.000000   \n",
       "211        0.755329       0.015188               29            0.995828   \n",
       "212        0.721038       0.020491              151            0.998609   \n",
       "213        0.715477       0.021863              172            1.000000   \n",
       "214        0.754402       0.007099               32            0.997218   \n",
       "215        0.708063       0.029685              186            1.000000   \n",
       "216        0.683967       0.010791              228            0.764951   \n",
       "217        0.709917       0.011565              180            0.827538   \n",
       "218        0.683967       0.010282              228            0.769124   \n",
       "219        0.682113       0.005046              233            0.764951   \n",
       "220        0.721038       0.009201              151            0.828929   \n",
       "221        0.673772       0.015979              240            0.770515   \n",
       "222        0.679333       0.007040              238            0.789986   \n",
       "223        0.717331       0.008457              166            0.845619   \n",
       "224        0.684893       0.009341              225            0.792768   \n",
       "225        0.681186       0.018238              234            0.789986   \n",
       "226        0.716404       0.011771              170            0.844228   \n",
       "227        0.684893       0.015175              225            0.792768   \n",
       "228        0.687674       0.008193              218            0.778860   \n",
       "229        0.721038       0.014773              151            0.827538   \n",
       "230        0.683967       0.011098              228            0.783032   \n",
       "231        0.686747       0.012526              223            0.773296   \n",
       "232        0.719184       0.004553              158            0.830320   \n",
       "233        0.687674       0.014886              218            0.778860   \n",
       "234        0.684893       0.011032              225            0.798331   \n",
       "235        0.722892       0.018019              146            0.845619   \n",
       "236        0.691381       0.011005              213            0.803894   \n",
       "237        0.679333       0.006136              237            0.798331   \n",
       "238        0.715477       0.012137              172            0.848401   \n",
       "239        0.678406       0.010784              239            0.801113   \n",
       "\n",
       "     split1_train_score  split2_train_score  mean_train_score  std_train_score  \n",
       "0              0.714882            0.729167          0.721034         0.005997  \n",
       "1              0.739917            0.761111          0.745127         0.011528  \n",
       "2              0.714882            0.727778          0.721035         0.005281  \n",
       "3              0.712100            0.729167          0.720107         0.007007  \n",
       "4              0.738526            0.762500          0.745126         0.012403  \n",
       "5              0.713491            0.730556          0.721034         0.007106  \n",
       "6              0.699583            0.719444          0.710376         0.008200  \n",
       "7              0.720445            0.751389          0.733541         0.013072  \n",
       "8              0.698192            0.719444          0.709449         0.008722  \n",
       "9              0.699583            0.719444          0.710376         0.008200  \n",
       "10             0.720445            0.750000          0.732615         0.012617  \n",
       "11             0.698192            0.719444          0.709912         0.008813  \n",
       "12             0.719054            0.741667          0.728910         0.009457  \n",
       "13             0.739917            0.761111          0.745127         0.011528  \n",
       "14             0.720445            0.743056          0.729836         0.009619  \n",
       "15             0.716273            0.741667          0.727519         0.010568  \n",
       "16             0.738526            0.762500          0.745126         0.012403  \n",
       "17             0.719054            0.741667          0.728910         0.009457  \n",
       "18             0.709318            0.733333          0.718251         0.010725  \n",
       "19             0.720445            0.751389          0.733541         0.013072  \n",
       "20             0.709318            0.731944          0.718251         0.009831  \n",
       "21             0.709318            0.736111          0.719640         0.011771  \n",
       "22             0.720445            0.750000          0.732615         0.012617  \n",
       "23             0.709318            0.734722          0.719641         0.010903  \n",
       "24             0.792768            0.784722          0.786841         0.004248  \n",
       "25             0.815021            0.816667          0.815569         0.000776  \n",
       "26             0.796940            0.783333          0.789159         0.005724  \n",
       "27             0.794159            0.784722          0.787768         0.004521  \n",
       "28             0.815021            0.819444          0.816032         0.002479  \n",
       "29             0.796940            0.784722          0.788695         0.005831  \n",
       "..                  ...                 ...               ...              ...  \n",
       "210            0.994437            0.995833          0.996757         0.002363  \n",
       "211            0.994437            0.993056          0.994440         0.001132  \n",
       "212            0.994437            0.995833          0.996293         0.001734  \n",
       "213            0.994437            0.994444          0.996294         0.002621  \n",
       "214            0.994437            0.993056          0.994904         0.001731  \n",
       "215            0.997218            0.994444          0.997221         0.002268  \n",
       "216            0.763561            0.758333          0.762282         0.002849  \n",
       "217            0.833102            0.843056          0.834565         0.006419  \n",
       "218            0.767733            0.762500          0.766452         0.002852  \n",
       "219            0.764951            0.759722          0.763208         0.002465  \n",
       "220            0.833102            0.847222          0.836418         0.007828  \n",
       "221            0.766342            0.766667          0.767841         0.001895  \n",
       "222            0.784423            0.779167          0.784525         0.004418  \n",
       "223            0.847010            0.854167          0.848932         0.003745  \n",
       "224            0.785814            0.777778          0.785453         0.006125  \n",
       "225            0.785814            0.779167          0.784989         0.004455  \n",
       "226            0.844228            0.856944          0.848467         0.005995  \n",
       "227            0.791377            0.773611          0.785919         0.008721  \n",
       "228            0.777469            0.776389          0.777572         0.001011  \n",
       "229            0.834492            0.844444          0.835492         0.006938  \n",
       "230            0.778860            0.783333          0.781742         0.002042  \n",
       "231            0.780250            0.775000          0.776182         0.002960  \n",
       "232            0.833102            0.844444          0.835955         0.006109  \n",
       "233            0.783032            0.783333          0.781742         0.002042  \n",
       "234            0.791377            0.790278          0.793329         0.003566  \n",
       "235            0.847010            0.850000          0.847543         0.001828  \n",
       "236            0.794159            0.798611          0.798888         0.003979  \n",
       "237            0.792768            0.794444          0.795181         0.002330  \n",
       "238            0.848401            0.854167          0.850323         0.002718  \n",
       "239            0.802503            0.794444          0.799354         0.003517  \n",
       "\n",
       "[240 rows x 25 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## printing out our results\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
